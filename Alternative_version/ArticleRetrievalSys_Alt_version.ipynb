{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCwuXCTYBToQ",
        "outputId": "e0bcd885-1100-4c1f-8feb-98a8a8a1c279"
      },
      "source": [
        "#!pip install -U chromadb langchain  rank_bm25 sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "awJvkb-pBSeB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DataAnalyzer:\n",
        "    def __init__(self,chunk_size=200,stop_words='english',alfa=0.5):\n",
        "        self.vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, length_function=len)\n",
        "        self.alfa = alfa\n",
        "\n",
        "    def analyze_data(self, query, data):\n",
        "        tfidf_matrix = self.calculate_tfidf_matrix(data)\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
        "        sorted_indices, sorted_accuracy = self.sort_similarity_scores(similarity_scores)\n",
        "\n",
        "        if self.check_accuracy_threshold(sorted_accuracy):\n",
        "            wall_index = self.determine_wall_index(sorted_accuracy)\n",
        "            selected_text = self.select_text(data, sorted_indices, wall_index)\n",
        "            chunked_data = self.split_text_into_chunks(selected_text)\n",
        "            to_final_selection = self.analyze_chunks(chunked_data, query)\n",
        "            final_ans = self.retrieve_final_documents(to_final_selection, query)\n",
        "            return \"\".join(final_ans)\n",
        "        else:\n",
        "            print(\"no article that fits well\")\n",
        "            return None\n",
        "\n",
        "    def calculate_tfidf_matrix(self, data):\n",
        "        return self.vectorizer.fit_transform(data[\"Text\"])\n",
        "\n",
        "    def sort_similarity_scores(self, similarity_scores):\n",
        "        sorted_indices = np.argsort(similarity_scores)[0][::-1]\n",
        "        sorted_accuracy = np.sort(similarity_scores)[0][::-1]\n",
        "        return sorted_indices, sorted_accuracy\n",
        "\n",
        "    def check_accuracy_threshold(self, sorted_accuracy):\n",
        "        biggest_accuracy = sorted_accuracy[0]\n",
        "        return biggest_accuracy >= 0.1\n",
        "\n",
        "    def determine_wall_index(self, sorted_accuracy):\n",
        "        wall_index = 1\n",
        "        wall_accuracy = sorted_accuracy[0] * self.alfa\n",
        "        for i in range(1, len(sorted_accuracy)):\n",
        "            if sorted_accuracy[i] < wall_accuracy:\n",
        "                break\n",
        "            wall_index += 1\n",
        "        return wall_index\n",
        "\n",
        "    def select_text(self, data, sorted_indices, wall_index):\n",
        "        article_to_search = sorted_indices[:wall_index]\n",
        "        selected_text = [data.iloc[i][\"Text\"] for i in article_to_search]\n",
        "        return selected_text\n",
        "\n",
        "    def split_text_into_chunks(self, selected_text):\n",
        "        chunked_data = [self.text_splitter.split_text(text) for text in selected_text]\n",
        "        return chunked_data\n",
        "\n",
        "    def analyze_chunks(self, chunked_data, query):\n",
        "        to_final_selection = []\n",
        "        for chunk in chunked_data:\n",
        "            chunk_vector = self.vectorizer.transform(chunk)\n",
        "            query_vector_chunk = self.vectorizer.transform([query])\n",
        "            similarity_scores_chunk = cosine_similarity(query_vector_chunk, chunk_vector)\n",
        "            sorted_accuracy_chunk = np.sort(similarity_scores_chunk)[0][::-1]\n",
        "            biggest_accuracy_chunk = sorted_accuracy_chunk[0]\n",
        "            wall_indexx = self.determine_wall_index(sorted_accuracy_chunk)\n",
        "            bm25_retriever = BM25Retriever.from_texts(chunk)\n",
        "            bm25_retriever.k = wall_index\n",
        "            to_final_selection += self.Doctranslate(bm25_retriever.get_relevant_documents(query))\n",
        "        return to_final_selection\n",
        "\n",
        "    def retrieve_final_documents(self, to_final_selection, query):\n",
        "        bm25_retriever_final = BM25Retriever.from_texts(to_final_selection)\n",
        "        bm25_retriever_final.k = wall_indexx\n",
        "        final_ans = self.Doctranslate(bm25_retriever_final.get_relevant_documents(query))\n",
        "        return final_ans\n",
        "\n",
        "    def Doctranslate(self, doc):\n",
        "        odp = []\n",
        "        for i in doc:\n",
        "            odp.append(i.page_content)\n",
        "        return odp\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HolIPCL-Hizp",
        "outputId": "0b8502f6-cd42-404a-c6a3-b16fb2a3f448"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'most sophisticated AI’s. Of course, it is a really complicated problem — it took evolution 4 Billion years to create humans and I still manage to look for my keys for ten minutes just to realize they'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "analyzer = DataAnalyzer()\n",
        "query = \"humans evolution\"\n",
        "data = pd.read_csv(\"/medium.csv\")  \n",
        "\n",
        "analyzer.analyze_data(query, data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
