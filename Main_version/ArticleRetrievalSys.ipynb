{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCwuXCTYBToQ",
        "outputId": "bfabcf48-709c-4232-cdd6-a06cd9db6059"
      },
      "outputs": [],
      "source": [
        "#!pip install -U chromadb langchain  rank_bm25 sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "awJvkb-pBSeB"
      },
      "outputs": [],
      "source": [
        "class ArticleSearch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        query: str,\n",
        "        first_chanker_size: int = 5000,\n",
        "        secount_chanker_size: int = 1000,\n",
        "        device: str = \"cuda\",\n",
        "        stop_words: str = \"english\",\n",
        "        alfa: float = 0.5,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.query = query\n",
        "        self.first_chanker_size = first_chanker_size\n",
        "        self.secount_chanker_size = secount_chanker_size\n",
        "        self.device = device\n",
        "        self.stop_words = stop_words\n",
        "        self.alfa = alfa\n",
        "        self.data: pd.DataFrame = None\n",
        "        self.bge_embeddings: HuggingFaceBgeEmbeddings = None\n",
        "        self.vectorstore: Chroma = None\n",
        "\n",
        "    def load_data(self, file_path: str) -> None:\n",
        "        self.data = pd.read_csv(file_path)\n",
        "\n",
        "    def preprocess_data(self) -> List[str]:\n",
        "        corpus = self.data[\"Title\"] + \" \" + self.data[\"Text\"]\n",
        "        return corpus.tolist()\n",
        "\n",
        "    def calculate_similarity(self, corpus: List[str]) -> np.ndarray:\n",
        "        vectorizer = TfidfVectorizer(stop_words=self.stop_words)\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "        query_vector = vectorizer.transform([self.query])\n",
        "        similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
        "        return similarity_scores\n",
        "\n",
        "    def search_best_article(self, similarity_scores: np.ndarray) -> List[int]:\n",
        "        sorted_indices = np.argsort(similarity_scores)[0][::-1]\n",
        "        sorted_accuracy = np.sort(similarity_scores)[0][::-1]\n",
        "        biggest_accuracy = sorted_accuracy[0]\n",
        "        if biggest_accuracy < 0.1:\n",
        "            print(\"no article that fits well\")\n",
        "            return []\n",
        "\n",
        "        wall_index = 1\n",
        "        wall_accuracy = biggest_accuracy * self.alfa\n",
        "        for i in range(1, len(sorted_accuracy)):\n",
        "            if sorted_accuracy[i] < wall_accuracy:\n",
        "                break\n",
        "            wall_index += 1\n",
        "        article_to_search = sorted_indices[:wall_index]\n",
        "        return article_to_search.tolist()\n",
        "\n",
        "    def retrieve_documents(self, article_indices: List[int]) -> List[Document]:\n",
        "        docs = []\n",
        "        for i in article_indices:\n",
        "            docs.append(Document(page_content=self.data.iloc[i][\"Text\"]))\n",
        "        return docs\n",
        "\n",
        "    def splitter_maker(self, size: int) -> RecursiveCharacterTextSplitter:\n",
        "        spliter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=size,\n",
        "            chunk_overlap=0,\n",
        "            length_function=len,\n",
        "        )\n",
        "        return spliter\n",
        "\n",
        "    def search_articles(self, file_path: str) -> str:\n",
        "        self.load_data(file_path)\n",
        "        corpus = self.preprocess_data()\n",
        "        similarity_scores = self.calculate_similarity(corpus)\n",
        "        article_to_search = self.search_best_article(similarity_scores)\n",
        "        if article_to_search:\n",
        "            self.bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "                model_name=self.model_name,\n",
        "                model_kwargs={\"device\": self.device},\n",
        "                encode_kwargs={\"normalize_embeddings\": True},\n",
        "            )\n",
        "            parent_splitter = self.splitter_maker(self.first_chanker_size)\n",
        "            child_splitter = self.splitter_maker(self.secount_chanker_size)\n",
        "            self.vectorstore = Chroma(\n",
        "                collection_name=\"split_parents\", embedding_function=self.bge_embeddings\n",
        "            )\n",
        "            store = InMemoryStore()\n",
        "            big_chunks_retriever = ParentDocumentRetriever(\n",
        "                vectorstore=self.vectorstore,\n",
        "                docstore=store,\n",
        "                child_splitter=child_splitter,\n",
        "                parent_splitter=parent_splitter,\n",
        "            )\n",
        "            docs = self.retrieve_documents(article_to_search)\n",
        "            big_chunks_retriever.add_documents(docs)\n",
        "            sub_docs = self.vectorstore.similarity_search(self.query)\n",
        "            return sub_docs[0].page_content\n",
        "        else:\n",
        "            return \"No article found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HolIPCL-Hizp",
        "outputId": "40525f79-abef-4b09-ffcc-57b0a46dc9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As such to summarise the two topics that I have explored now each in 50 days I would say the following.\n",
            "\n",
            "Artificial intelligence (AI) is a loved and hated concept, an ambiguous suitcase word that is in no way close to being human or not human. A human is more than a series of programmed actions, yet we are also this. Questions are arising of sustainability in regards to artificial intelligence alongside new techniques in machine learning. The climate crisis can be worsened by irresponsible use of technology, particularly when large technology companies self-police or self-evaluate.\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "query =  \"Artificial Intelligence\" #query to search\n",
        "file_path = \"medium.csv\"\n",
        "\n",
        "article_search = ArticleSearch(model_name, query)\n",
        "article_content = article_search.search_articles(file_path)\n",
        "print(article_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
